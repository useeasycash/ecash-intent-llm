inference:
  max_new_tokens: 512
  temperature: 0.1
  top_p: 0.95
  repetition_penalty: 1.15

server:
  host: "0.0.0.0"
  port: 8000
  workers: 4
  concurrency_limit: 10

logging:
  level: "INFO"
  json_format: true
